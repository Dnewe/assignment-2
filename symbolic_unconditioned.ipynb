{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "82b7bdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import pretty_midi\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import music21"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb379ce",
   "metadata": {},
   "source": [
    "## DATA ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3865f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path=\"maestro-v3.0.0.csv\",\n",
    "midi_root_dir=\"maestro-v3.0.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf59671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e9c008",
   "metadata": {},
   "source": [
    "## DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "06aeaec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, sequences, max_seq_len=512):\n",
    "        self.data = []\n",
    "        for seq in sequences:\n",
    "            if max_seq_len is not None:\n",
    "                # Break long sequences into chunks of max_seq_len\n",
    "                for i in range(0, len(seq), max_seq_len):\n",
    "                    chunk = seq[i:i + max_seq_len]\n",
    "                    if len(chunk) > 1:\n",
    "                        self.data.append(torch.tensor(chunk, dtype=torch.long))\n",
    "            else:\n",
    "                self.data.append(torch.tensor(seq, dtype=torch.long))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    batch = pad_sequence(batch, batch_first=False, padding_value=0)\n",
    "    return batch\n",
    "\n",
    "\n",
    "class NoteTokenizer:\n",
    "    def __init__(self):\n",
    "        self.token_to_id = {}\n",
    "        self.id_to_token = {}\n",
    "\n",
    "    def build_vocab(self, sequences):\n",
    "        unique_tokens = sorted(set(token for seq in sequences for token in seq))\n",
    "        self.token_to_id = {token: i+1 for i, token in enumerate(unique_tokens)}  # 0 = padding\n",
    "        self.id_to_token = {i: token for token, i in self.token_to_id.items()}\n",
    "\n",
    "    def encode(self, sequence):\n",
    "        return [self.token_to_id[token] for token in sequence if token in self.token_to_id]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return [self.id_to_token[i] for i in ids if i in self.id_to_token]\n",
    "\n",
    "    def vocab_size(self):\n",
    "        return len(self.token_to_id) + 1  # add padding token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "94eb6eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_key_signature(midi_path):\n",
    "        \"\"\"Detect key signature using music21.\"\"\"\n",
    "        try:\n",
    "            score = music21.converter.parse(midi_path)\n",
    "            key_sig = score.analyze('key')\n",
    "            return f\"{key_sig.tonic.name}_{key_sig.mode}\"\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4ab0ec19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def notes_to_tokens(notes, key_sig, dur_step=0.05, time_step=0.05):\n",
    "    \"\"\"\n",
    "    notes: List[(start, pitch, duration)]\n",
    "    returns: List[str] of tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    prev_start = 0.0\n",
    "    for start, pitch, dur in notes:\n",
    "        dt = round((start - prev_start) / time_step) * time_step\n",
    "        d = round(dur / dur_step) * dur_step\n",
    "        prev_start = start\n",
    "\n",
    "        tokens.append(f\"TIME_SHIFT_{dt:.2f}\")\n",
    "        tokens.append(f\"NOTE_ON_{pitch}\")\n",
    "        tokens.append(f\"DURATION_{d:.2f}\")\n",
    "        tokens.append(f\"KEY_{key_sig}\")\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def extract_notes_from_midi(midi_path):\n",
    "    try:\n",
    "        pm = pretty_midi.PrettyMIDI(midi_path)\n",
    "        notes = []\n",
    "        for instr in pm.instruments:\n",
    "            if instr.is_drum: continue\n",
    "            for n in instr.notes:\n",
    "                notes.append((n.start, n.pitch, n.end - n.start))\n",
    "        notes.sort(key=lambda x: x[0])\n",
    "        return notes\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {midi_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def prepare_token_seqs(csv_path, midi_root_dir, tokenizer):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    splits = {'train': [], 'validation': [], 'test': []}\n",
    "\n",
    "    for split in splits:\n",
    "        files = df[df['split'] == split]['midi_filename']\n",
    "        for fname in files:\n",
    "            path = os.path.join(midi_root_dir, fname)\n",
    "            notes = extract_notes_from_midi(path)\n",
    "            key_sig = detect_key_signature(path)\n",
    "            if len(notes) > 20:\n",
    "                splits[split].append((notes, key_sig))\n",
    "    \n",
    "    train_tok_seqs = [\n",
    "        notes_to_tokens(notes, key_sig)\n",
    "        for notes, key_sig in splits['train']\n",
    "    ]\n",
    "    tokenizer.build_vocab(train_tok_seqs)\n",
    "\n",
    "    def encode_split(raw_seqs):\n",
    "        out = []\n",
    "        for notes in raw_seqs:\n",
    "            toks = notes_to_tokens(notes)\n",
    "            ids = tokenizer.encode(toks)\n",
    "            if len(ids) > 1:\n",
    "                out.append(ids)\n",
    "        return out\n",
    "\n",
    "    return (\n",
    "        encode_split(splits['train']),\n",
    "        encode_split(splits['validation']),\n",
    "        encode_split(splits['test'])\n",
    "    )\n",
    "\n",
    "\n",
    "def make_loaders(train_seqs, val_seqs, test_seqs, batch_size=16, max_seq_len=512):\n",
    "    train_ds = MusicDataset(train_seqs, max_seq_len)\n",
    "    val_ds   = MusicDataset(val_seqs,   max_seq_len)\n",
    "    test_ds  = MusicDataset(test_seqs,  max_seq_len)\n",
    "    return (\n",
    "        DataLoader(train_ds, batch_size=batch_size, shuffle=True, collate_fn=collate_fn),\n",
    "        DataLoader(val_ds,   batch_size=batch_size, shuffle=False, collate_fn=collate_fn),\n",
    "        DataLoader(test_ds,  batch_size=batch_size, shuffle=False, collate_fn=collate_fn),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5b9ae5",
   "metadata": {},
   "source": [
    "## MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "797904d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=64, nhead=2, num_layers=2, dim_ff=128, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead,\n",
    "                                                   dim_feedforward=dim_ff, dropout=dropout)\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src: (seq_len, batch)\n",
    "        x = self.token_embedding(src)  # (seq_len, batch, d_model)\n",
    "        x = self.positional_encoding(x)  # (seq_len, batch, d_model)\n",
    "        x = self.transformer(x)  # (seq_len, batch, d_model)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))  # (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "da24ade8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MusicTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=256, nhead=8, num_layers=6, dim_feedforward=512, dropout=0.1):\n",
    "        super(MusicTransformer, self).__init__()\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_layer = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (seq_len, batch_size)\n",
    "        x = self.token_embedding(x)  # (seq_len, batch_size, d_model)\n",
    "        x = self.positional_encoding(x)\n",
    "        x = self.transformer(x)\n",
    "        logits = self.output_layer(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-torch.log(torch.tensor(10000.0)) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580e949d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=2048):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * -(torch.log(torch.tensor(10000.0)) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div)\n",
    "        pe[:, 1::2] = torch.cos(position * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(1))  # (max_len, 1, d_model)\n",
    "\n",
    "    def forward(self, x):  # x: (seq_len, batch, d_model)\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "class ConformerBlock(nn.Module):\n",
    "    def __init__(self, d_model, nhead, dim_ff, conv_kernel_size=31, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Feed-forward module 1\n",
    "        self.ffn1 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Multi-head self-attention\n",
    "        self.mha = nn.MultiheadAttention(d_model, nhead, dropout=dropout)\n",
    "        self.ln_attn = nn.LayerNorm(d_model)\n",
    "        # Convolution module\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Conv1d(d_model, 2 * d_model, kernel_size=1),\n",
    "            nn.GLU(dim=1),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=conv_kernel_size, padding=conv_kernel_size//2, groups=1),\n",
    "            nn.BatchNorm1d(d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Conv1d(d_model, d_model, kernel_size=1),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Feed-forward module 2\n",
    "        self.ffn2 = nn.Sequential(\n",
    "            nn.LayerNorm(d_model),\n",
    "            nn.Linear(d_model, dim_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(dim_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        # Final layer norm\n",
    "        self.ln_final = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x):  # x: (seq_len, batch, d_model)\n",
    "        # FFN1 with residual\n",
    "        x = x + 0.5 * self.ffn1(x)\n",
    "        # Self-attention with residual\n",
    "        residual = x\n",
    "        x = self.ln_attn(x)\n",
    "        x2, _ = self.mha(x, x, x)\n",
    "        x = residual + x2\n",
    "        # Convolution module with residual\n",
    "        residual = x\n",
    "        # reshape for conv: (batch, d_model, seq_len)\n",
    "        x_conv = x.permute(1, 2, 0)\n",
    "        x_conv = self.conv(x_conv)\n",
    "        x = residual + x_conv.permute(2, 0, 1)\n",
    "        # FFN2 with residual\n",
    "        x = x + 0.5 * self.ffn2(x)\n",
    "        # final norm\n",
    "        return self.ln_final(x)\n",
    "\n",
    "class MusicConformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=4, dim_ff=256,\n",
    "                 conv_kernel_size=31, dropout=0.1, max_len=2048):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=0)\n",
    "        self.pos_enc = PositionalEncoding(d_model, dropout, max_len)\n",
    "        self.layers = nn.ModuleList([\n",
    "            ConformerBlock(d_model, nhead, dim_ff, conv_kernel_size, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src):  # src: (seq_len, batch)\n",
    "        x = self.embedding(src)  # (seq_len, batch, d_model)\n",
    "        x = self.pos_enc(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output(x)  # (seq_len, batch, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "887ebdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_model(model, train_loader, val_loader, device, epochs=10, lr=1e-4):\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_model = copy.deepcopy(model)\n",
    "    best_val_loss = 1e8\n",
    "    for epoch in range(epochs):\n",
    "        # train\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for batch in tqdm(train_loader):\n",
    "            batch = batch.to(device)\n",
    "            src = batch[:-1] \n",
    "            tgt = batch[1:] \n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = loss_fn(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            #print(f\"Epoch {epoch+1}/{epochs} | Batch {i}/{len(train_loader)} | Train Loss: {loss.item():.4f}\")\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        \n",
    "        # validation\n",
    "        model.eval()\n",
    "        total_val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                batch = batch.to(device)\n",
    "                src = batch[:-1]\n",
    "                tgt = batch[1:]\n",
    "\n",
    "                output = model(src)\n",
    "                loss = loss_fn(output.view(-1, output.size(-1)), tgt.view(-1))\n",
    "                total_val_loss += loss.item()\n",
    "        avg_val_loss = total_val_loss / len(val_loader)\n",
    "\n",
    "        # save best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_model = copy.deepcopy(model)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "    \n",
    "    return best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae73370",
   "metadata": {},
   "source": [
    "## TRAINING PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c9a2dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing data...\n"
     ]
    }
   ],
   "source": [
    "# DATA PROCESSING\n",
    "print(\"Preparing data...\")\n",
    "tokenizer = NoteTokenizer()\n",
    "train_ids, val_ids, test_ids = prepare_token_seqs(\n",
    "    csv_path=\"maestro-v3.0.0.csv\",\n",
    "    midi_root_dir=\"maestro-v3.0.0\",\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfcec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n"
     ]
    }
   ],
   "source": [
    "# DATALOADERS\n",
    "BATCH_SIZE = 128\n",
    "MAX_SEQ_LEN = 128\n",
    "\n",
    "TRAIN_SIZE = 100 # for faster training\n",
    "\n",
    "train_loader, val_loader, test_loader = make_loaders(train_ids[:TRAIN_SIZE], val_ids, test_ids, max_seq_len=MAX_SEQ_LEN, batch_size=BATCH_SIZE)\n",
    "\n",
    "print(tokenizer.vocab_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00026d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing model...\n",
      "Training...\n",
      "train size: 234\n",
      "validation size: 118\n",
      "test size: 137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/234 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|â–Ž         | 7/234 [00:03<02:00,  1.88it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mvalidation size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(val_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtest size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcpu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLEARNING_RATE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, train_loader, val_loader, device, epochs, lr)\u001b[39m\n\u001b[32m     16\u001b[39m output = model(src)\n\u001b[32m     17\u001b[39m loss = loss_fn(output.view(-\u001b[32m1\u001b[39m, output.size(-\u001b[32m1\u001b[39m)), tgt.view(-\u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m optimizer.step()\n\u001b[32m     21\u001b[39m total_train_loss += loss.item()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\torch\\autograd\\graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# MODEL TRAINING\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 5e-4\n",
    "\n",
    "print(\"Initializing model...\")\n",
    "model = SimpleMusicTransformer(vocab_size=tokenizer.vocab_size())\n",
    "#model = MusicConformer(vocab_size=tokenizer.vocab_size())\n",
    "\n",
    "print(\"Training...\")\n",
    "print(f\"train size: {len(train_loader)}\")\n",
    "print(f\"validation size: {len(val_loader)}\")\n",
    "print(f\"test size: {len(test_loader)}\")\n",
    "model = train_model(model, train_loader, val_loader, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "            epochs=EPOCHS, lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69366656",
   "metadata": {},
   "source": [
    "## DECODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e21227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_tokens(ids, tokenizer):\n",
    "    return tokenizer.decode(ids)\n",
    "\n",
    "\n",
    "def sample_from_model(model, seed_ids, length=100, device=\"cpu\", temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate sample from model using a seed.\n",
    "    \n",
    "    Args:\n",
    "        model: model\n",
    "        seed_ids: list of ids referring to tokens\n",
    "        length: number of notes played\n",
    "        device: device used to generate sample\n",
    "        temperature: temperature for logits \n",
    "    \n",
    "    Returns:\n",
    "        List of generated tokens\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    generated = seed_ids[:]\n",
    "    input_seq = torch.tensor(generated, dtype=torch.long).unsqueeze(1).to(device)  # (seq_len, 1)\n",
    "\n",
    "    for _ in range(length):\n",
    "        with torch.no_grad():\n",
    "            output = model(input_seq)\n",
    "            logits = output[-1, 0] / temperature\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "        generated.append(next_token)\n",
    "        input_seq = torch.tensor(generated, dtype=torch.long).unsqueeze(1).to(device)\n",
    "\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "79ca6291",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokens_to_notes(tokens):\n",
    "    \"\"\"\n",
    "    Convert token sequence back to notes format.\n",
    "    \n",
    "    Args:\n",
    "        tokens: List of token strings\n",
    "    \n",
    "    Returns:\n",
    "        List of (start_time, pitch, duration) tuples\n",
    "    \"\"\"\n",
    "    notes = []\n",
    "    current_time = 0.0\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        token = tokens[i]\n",
    "        if token.startswith(\"TIME_SHIFT_\"):\n",
    "            try:\n",
    "                time_shift = float(token.replace(\"TIME_SHIFT_\", \"\"))\n",
    "                current_time += time_shift\n",
    "            except ValueError:\n",
    "                pass\n",
    "            i += 1\n",
    "        elif token.startswith(\"NOTE_ON_\") and i + 1 < len(tokens):\n",
    "            try:\n",
    "                pitch = int(token.replace(\"NOTE_ON_\", \"\"))\n",
    "                duration_token = tokens[i + 1]\n",
    "                \n",
    "                if duration_token.startswith(\"DURATION_\"):\n",
    "                    duration = float(duration_token.replace(\"DURATION_\", \"\"))\n",
    "                    \n",
    "                    # Ensure valid ranges\n",
    "                    if 0 <= pitch <= 127 and duration > 0:\n",
    "                        notes.append((current_time, pitch, duration))\n",
    "                    \n",
    "                    i += 2\n",
    "                else:\n",
    "                    i += 1\n",
    "            except (ValueError, IndexError):\n",
    "                i += 1\n",
    "        else:\n",
    "            i += 1\n",
    "    return notes\n",
    "\n",
    "\n",
    "def notes_to_midi(notes , \n",
    "                  output_path , \n",
    "                  program = 0,\n",
    "                  tempo = 120.0):\n",
    "    \"\"\"\n",
    "    Convert notes to MIDI file using pretty_midi.\n",
    "    \n",
    "    Args:\n",
    "        notes: List of (start_time, pitch, duration) tuples\n",
    "        output_path: Path to save MIDI file\n",
    "        program: MIDI program number (instrument)\n",
    "        tempo: Tempo in BPM\n",
    "    \"\"\"\n",
    "    midi = pretty_midi.PrettyMIDI(initial_tempo=tempo)\n",
    "    instrument = pretty_midi.Instrument(program=program)\n",
    "    \n",
    "    for start_time, pitch, duration in notes:\n",
    "        pitch = max(0, min(127, int(pitch)))\n",
    "        \n",
    "        note = pretty_midi.Note(\n",
    "            velocity=80,\n",
    "            pitch=pitch,\n",
    "            start=float(start_time),\n",
    "            end=float(start_time + duration)\n",
    "        )\n",
    "        instrument.notes.append(note)\n",
    "    \n",
    "    # Add instrument to MIDI\n",
    "    midi.instruments.append(instrument)\n",
    "    \n",
    "    # Save MIDI file\n",
    "    midi.write(output_path)\n",
    "    print(f\"MIDI file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e324b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0.0, 66, 1.2), (0.0, 42, 1.2), (2.55, 73, 0.6), (3.3499999999999996, 61, 0.6), (3.8999999999999995, 61, 0.6), (4.449999999999999, 61, 0.6), (5.249999999999999, 66, 1.2), (5.799999999999999, 66, 1.2), (6.349999999999999, 66, 1.2), (6.899999999999999, 66, 1.2)]\n",
      "MIDI file saved to: generated.mid\n"
     ]
    }
   ],
   "source": [
    "# DECODING PARAMS\n",
    "TEMPERATURE = 1.05\n",
    "\n",
    "model.eval()\n",
    "generated_ids = sample_from_model(model, seed_ids=[60,62,64], length=200, temperature=TEMPERATURE)\n",
    "tokens = decode_tokens(generated_ids, tokenizer)\n",
    "events = tokens_to_notes(tokens)\n",
    "print(events[:10])\n",
    "notes_to_midi(events, \"generated.mid\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
